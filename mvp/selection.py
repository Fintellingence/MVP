from functools import partial
from multiprocessing import Pool

import numpy as np
import pandas as pd


def parallel_map_df(func, data, num_of_threads, num_of_chunks, **kwargs):
    """
    Apply `func` in linear distributed chunks of the data in `data`
    using parallel processing.

    Parameters
    ----------
    `func` : ``Called``
        A function to be applied in along the data chunks. It must return a
        ``DataFrame``.
    `data` : ``[Index]``
        The data that will be divided in different chunks.
    ``num_of_threads : ``int``
        The number of threads that will process the chunks.
    ``num_of_chunks : ``int``
        The number of chunks.
    ``**kwargs : ``dict``
        Addicional arguments that will be passed to the `func`.

    Return
    ------
    `df_out` : ``DataFrame``
        The ``DataFrame`` generated by the application of `func` in
        `data` with the arguments in `**kwargs`.
    """

    def slice_data(chunks, data):
        n_chunks = chunks + 1 if chunks < len(data) else len(data)
        chunk_idx = np.floor(np.linspace(0, len(data), n_chunks)).astype(int)
        for i in range(1, chunk_idx.size):
            yield data[chunk_idx[i - 1] : chunk_idx[i]]

    slicer = slice_data(num_of_chunks, data)
    partial_func = partial(func, **kwargs)
    with Pool(num_of_threads) as pool:
        output = [out for out in pool.imap(partial_func, slicer)]
    pd_out = pd.concat(output, axis=0)
    pd_out = pd_out.loc[~pd_out.index.duplicated(keep="last")].sort_index()
    return pd_out


def chunk_count_occurrences(event_chunks, closed_index, horizon):
    """
    Determine the number of occurrences of horizons in `event_chunks`.

    Parameters
    ----------
    `closed_index` : ``Index``
        The sorted timestamps of the closed prices
    `horizon` : ``Series``
        The start and end of each horizon
    `event_chunks` : ``[list, Series, Index]``
        The timestamps that compose the events of interest

    Return
    ------
    count : ``Series``
        The number of occurrence of the `event_chunks` in all horizons
    """
    horizon = horizon[: event_chunks[-1]]
    horizon = horizon[horizon >= event_chunks[0]]
    idx_of_interest = closed_index.searchsorted(
        [horizon.index[0], horizon.max()]
    )
    count = pd.Series(
        0, index=closed_index[idx_of_interest[0] : idx_of_interest[1] + 1]
    )
    for s, e in horizon.iteritems():
        count.loc[s:e] += 1
    return count.loc[event_chunks[0] : horizon[event_chunks].max()]


def chunk_avg_uniqueness(event_chunks, horizon, occurrences):
    """
    Determine the average uniqueness of `horizon`, i.e., the average
    uniqueness of labels, in `event_chunks`.

    Parameters
    ----------
    `horizon` : ``Series``
        The start and end of each horizon
    `occurrences` : ``Series``
        The number of occurrence of all horizons in all events
        (see ``chunk_count_occurrences``)
    `event_chunks` : ``[list, Series, Index]``
        The timestamps that compose the events of interest

    Return
    ------
    avg_uniqueness : ``Series``
        Average uniquess associated with `horizon` in `event_chunks`
    """
    avg_uniqueness = pd.Series(index=event_chunks)
    for s, e in horizon.loc[event_chunks].iteritems():
        avg_uniqueness.loc[s] = (1.0 / occurrences.loc[s:e]).mean()
    return avg_uniqueness


def chunk_sample_weights(event_chunks, occurrences, horizon, closed_prices):
    """
    Determine the weights based on returns and number of instant overlaps,
    for horizons that start into `event_chunks`.

    Parameters
    ----------
    `occurrences` : ``Series``
        The number of occurrence of all horizons in all events
        (see ``event_chunks``)
    `horizon` : ``Series``
        The start and end of each horizon.
    `closed_prices` : ``Series``
        Closed prices for a symbol, index is composed of time instants.
    `event_chunks` : ``[list, Series, Index]``
        The timestamps that compose the events of interest

    Return
    ------
    weights : ``Series``
        Weights for `event_chunks`
    """
    log_return = np.log(closed_prices).diff()
    weights = pd.Series(index=event_chunks)
    horizon = horizon.loc[event_chunks]
    for s, e in horizon.iteritems():
        weights[s] = (log_return.loc[s:e] / occurrences.loc[s:e]).sum()
    return weights.abs()


def count_occurrences(closed_index, horizon, num_of_threads, num_of_chunks):
    """
    Compute all occurrences into the event space.
    """
    events = horizon.index
    occurrences = parallel_map_df(
        chunk_count_occurrences,
        events,
        num_of_threads,
        num_of_chunks,
        horizon=horizon,
        closed_index=closed_index,
    )
    occurrences = occurrences.reindex(closed_index).fillna(0)
    return occurrences


def avg_uniqueness(occurrences, horizon, num_of_threads, num_of_chunks):
    """
    Compute all average uniqueness into the event space.
    """
    events = horizon.index
    avg_uniqueness = parallel_map_df(
        chunk_avg_uniqueness,
        events,
        num_of_threads,
        num_of_chunks,
        horizon=horizon,
        occurrences=occurrences,
    )
    return avg_uniqueness


def sample_weights(
    occurrences, horizon, closed_prices, num_of_threads, num_of_chunks
):
    """
    Compute weights for all events
    """
    events = horizon.index
    sample_weights = parallel_map_df(
        chunk_sample_weights,
        events,
        num_of_threads,
        num_of_chunks,
        horizon=horizon,
        occurrences=occurrences,
        closed_prices=closed_prices,
    )
    return sample_weights


def time_weights(avg_uniqueness, p=1):
    """
    Determine the weights based on time evolution (the newest ones have greater
    weights). The time evolution is represented by the cummulative sum of
    average uniqueness for all events.

    Parameters
    ----------
    `avg_uniqueness` : ``Series``
        The average uniqueness for all events

    Return
    ------
    weights : ``Series``
        Weights for time evolution based on average uniqueness
    """
    weighted_time = avg_uniqueness.sort_index().cumsum()
    a = (1.0 - p) / weighted_time.iloc[-1]
    b = 1.0 - a * weighted_time.iloc[-1]
    weights = a * weighted_time + b
    weights[weights < 0] = 0
    return weights


def indicator(closed_idx, horizon):
    """
    Determine a matrix to indicate of occurrences, in which rows
    represent the timestamps and columns represent the events
    (aka horizons).
    """
    indicator = pd.DataFrame(
        0, index=closed_idx, columns=range(horizon.shape[0])
    )
    for i, (s, e) in enumerate(horizon.iteritems()):
        indicator.loc[s:e, i] = 1.0
    return indicator


def bootstrap_avg_uniqueness(indicator):
    """
    Determine the average uniqueness for the events represented in
    indicator matrix.
    """
    sum_occurrences = indicator.sum(axis=1)
    uniqueness = indicator.div(sum_occurrences, axis=0)
    return uniqueness[uniqueness > 0].mean()


def bootstrap_selection(indicator, num_of_data=None, seed=12345):
    """
    Select `num_of_data` horizons by sampling with replacement
    (aka bootstrap sampling) sequentialy. The samples are drawn from a
    discrete distribution, in which the probabilities of occurrence of each
    horizon rely on the number of overlapping with the past sampled
    horizons.

    Parameters
    ----------
    `indicator` : ``DataFrame``
        The matrix to indicate of occurrences of horizons along the time
        space of closed prices

    Return
    ------
    data_idx : ``list``
        The list contaning the indices for selected events (aka horizons)
    """
    data_idx = []
    rng = np.random.default_rng(seed)
    num_of_events = indicator.shape[1]
    num_of_data = num_of_events if num_of_data is None else num_of_data
    for _ in range(num_of_data):
        avg_uniqueness = []
        for i in range(num_of_events):
            occurrences_of_interest = indicator[data_idx + [i]]
            avg_uniqueness.append(
                bootstrap_avg_uniqueness(occurrences_of_interest)
            )
        prob = avg_uniqueness / avg_uniqueness.sum()
        data_idx.append(rng.choice(indicator.columns, p=prob))
    return data_idx


def cross_validation():
    pass
