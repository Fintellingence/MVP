from functools import partial
from multiprocessing import Pool
from sklearn.model_selection import KFold, StratifiedKFold

import numpy as np
import pandas as pd


def parallel_map_df(func, data, num_of_threads, num_of_chunks, **kwargs):
    """
    Apply `func` in linear distributed chunks of the data in `data`
    using parallel processing.

    Parameters
    ----------
    `func` : ``Called``
        A function to be applied in along the data chunks. It must return a
        ``DataFrame``.
    `data` : ``[Index]``
        The data that will be divided in different chunks.
    ``num_of_threads : ``int``
        The number of threads that will process the chunks.
    ``num_of_chunks : ``int``
        The number of chunks.
    ``**kwargs : ``dict``
        Addicional arguments that will be passed to the `func`.

    Return
    ------
    `df_out` : ``DataFrame``
        The ``DataFrame`` generated by the application of `func` in
        `data` with the arguments in `**kwargs`.
    """

    def slice_data(chunks, data):
        n_chunks = chunks + 1 if chunks < len(data) else len(data)
        chunk_idx = np.floor(np.linspace(0, len(data), n_chunks)).astype(int)
        for i in range(1, chunk_idx.size):
            yield data[chunk_idx[i - 1] : chunk_idx[i]]

    slicer = slice_data(num_of_chunks, data)
    partial_func = partial(func, **kwargs)
    with Pool(num_of_threads) as pool:
        output = [out for out in pool.imap(partial_func, slicer)]
    pd_out = pd.concat(output, axis=0)
    pd_out = pd_out.loc[~pd_out.index.duplicated(keep="last")].sort_index()
    return pd_out


def chunk_count_occurrences(event_chunks, closed_index, horizon):
    """
    Determine the number of occurrences of horizons in `event_chunks`.

    Parameters
    ----------
    `closed_index` : ``Index``
        The sorted timestamps of the closed prices
    `horizon` : ``Series``
        The start and end of each horizon
    `event_chunks` : ``[list, Series, Index]``
        The timestamps that compose the events of interest

    Return
    ------
    count : ``Series``
        The number of occurrence of the `event_chunks` in all horizons
    """
    horizon = horizon[: event_chunks[-1]]
    horizon = horizon[horizon >= event_chunks[0]]
    idx_of_interest = closed_index.searchsorted(
        [horizon.index[0], horizon.max()]
    )
    count = pd.Series(
        0, index=closed_index[idx_of_interest[0] : idx_of_interest[1] + 1]
    )
    for s, e in horizon.iteritems():
        count.loc[s:e] += 1
    return count.loc[event_chunks[0] : horizon[event_chunks].max()]


def chunk_avg_uniqueness(event_chunks, horizon, occurrences):
    """
    Determine the average uniqueness of `horizon`, i.e., the average
    uniqueness of labels, in `event_chunks`.

    Parameters
    ----------
    `horizon` : ``Series``
        The start and end of each horizon
    `occurrences` : ``Series``
        The number of occurrence of all horizons in all events
        (see ``chunk_count_occurrences``)
    `event_chunks` : ``[list, Series, Index]``
        The timestamps that compose the events of interest

    Return
    ------
    avg_uniqueness : ``Series``
        Average uniquess associated with `horizon` in `event_chunks`
    """
    avg_uniqueness = pd.Series(index=event_chunks)
    for s, e in horizon.loc[event_chunks].iteritems():
        avg_uniqueness.loc[s] = (1.0 / occurrences.loc[s:e]).mean()
    return avg_uniqueness


def chunk_sample_weights(event_chunks, occurrences, horizon, closed_prices):
    """
    Determine the weights based on returns and number of instant overlaps,
    for horizons that start into `event_chunks`.

    Parameters
    ----------
    `occurrences` : ``Series``
        The number of occurrence of all horizons in all events
        (see ``event_chunks``)
    `horizon` : ``Series``
        The start and end of each horizon.
    `closed_prices` : ``Series``
        Closed prices for a symbol, index is composed of time instants.
    `event_chunks` : ``[list, Series, Index]``
        The timestamps that compose the events of interest

    Return
    ------
    weights : ``Series``
        Weights for `event_chunks`
    """
    log_return = np.log(closed_prices).diff()
    weights = pd.Series(index=event_chunks)
    horizon = horizon.loc[event_chunks]
    for s, e in horizon.iteritems():
        weights[s] = (log_return.loc[s:e] / occurrences.loc[s:e]).sum()
    return weights.abs()


def count_occurrences(closed_index, horizon, num_of_threads, num_of_chunks):
    """
    Compute all occurrences into the event space.
    """
    events = horizon.index
    occurrences = parallel_map_df(
        chunk_count_occurrences,
        events,
        num_of_threads,
        num_of_chunks,
        horizon=horizon,
        closed_index=closed_index,
    )
    occurrences = occurrences.reindex(closed_index).fillna(0)
    return occurrences


def avg_uniqueness(occurrences, horizon, num_of_threads, num_of_chunks):
    """
    Compute all average uniqueness into the event space.
    """
    events = horizon.index
    avg_uniqueness = parallel_map_df(
        chunk_avg_uniqueness,
        events,
        num_of_threads,
        num_of_chunks,
        horizon=horizon,
        occurrences=occurrences,
    )
    return avg_uniqueness


def sample_weights(
    occurrences, horizon, closed_prices, num_of_threads, num_of_chunks
):
    """
    Compute weights for all events
    """
    events = horizon.index
    sample_weights = parallel_map_df(
        chunk_sample_weights,
        events,
        num_of_threads,
        num_of_chunks,
        horizon=horizon,
        occurrences=occurrences,
        closed_prices=closed_prices,
    )
    return sample_weights


def time_weights(avg_uniqueness, p=1, min_ratio=0.01, sort=False):
    """
    Determine the weights based on time evolution (the newest ones have larger
    weights). The time evolution is represented by the cummulative sum of
    average uniqueness for all events.

    Parameters
    ----------
    `avg_uniqueness` : ``Series``
        The average uniqueness for all events
    `p` : ``float``
        The line parameter in [0,1]
    `min_ratio` : ``float``
        The ratio to be multiplied by the maximum weitgh
    `sort` : ``bool``
        The inidication to sort the `avg_uniqueness` Serie

    Return
    ------
    weights : ``Series``
        Weights for time evolution based on average uniqueness
    """
    if sort:
        avg_uniqueness = avg_uniqueness.sort_index()
    weighted_time = avg_uniqueness.cumsum()
    a = (1.0 - p) / weighted_time.iloc[-1]
    b = 1.0 - a * weighted_time.iloc[-1]
    weights = a * weighted_time + b
    min_weight = weights.max() * min_ratio
    weights[weights < 0] = min_weight
    return weights


def indicator(closed_idx, horizon):
    """
    Determine a matrix to indicate of occurrences, in which rows
    represent the timestamps and columns represent the events.
    """
    indicator = pd.DataFrame(
        0, index=closed_idx, columns=range(horizon.shape[0])
    )
    for i, (s, e) in enumerate(horizon.iteritems()):
        indicator.loc[s:e, i] = 1.0
    return indicator


def indicator_avg_uniqueness(indicator):
    """
    Determine the average uniqueness for the events represented in
    indicator matrix.
    """
    sum_occurrences = indicator.sum(axis=1)
    uniqueness = indicator.div(sum_occurrences, axis=0)
    return uniqueness[uniqueness > 0].mean()


def bootstrap_selection(indicator, num_of_data=None, seed=12345):
    """
    Select `num_of_data` horizons by sampling with replacement
    (aka bootstrap sampling) sequentialy. The samples are drawn from a
    discrete distribution, in which the probabilities of occurrence of each
    horizon rely on the number of overlapping with the past sampled
    horizons.

    Parameters
    ----------
    `indicator` : ``DataFrame``
        The matrix to indicate of occurrences of horizons along the time
        space of closed prices

    Return
    ------
    data_idx : ``np.array``
        The list contaning the indices for selected events
    """
    rng = np.random.default_rng(seed)
    num_of_events = indicator.shape[1]
    num_of_data = num_of_events if num_of_data is None else num_of_data
    sampled_events = np.zeros(num_of_data)
    for i in range(num_of_data):
        avg_uniqueness = np.zeros(num_of_events)
        for j in range(num_of_events):
            sampled_events[i] = j
            occurrences_of_interest = indicator[sampled_events[0 : i + 1]]
            last_event_uniqueness = indicator_avg_uniqueness(
                occurrences_of_interest
            )
            last_event_uniqueness = last_event_uniqueness.values[-1]
            avg_uniqueness[j] = last_event_uniqueness
        prob = avg_uniqueness / avg_uniqueness.sum()
        sampled_events[i] = rng.choice(indicator.columns, p=prob)
    return sampled_events


# FIXME: Not Good
class _BasePEKFold:
    """
    Base class for cross validation based on K-fold for both non-stratified
    and stratified
    """

    def _purge_embargo_squencial_with_test(self, horizon, train_idx, test_idx):
        """
        Apply the purge and embargo processes to the `train_idx` based on the
        `test_idx` with sequencial indices representing a range of test events.

        Parameters
        ----------
        `train_idx`: ``np.array``
            Array composed of intergers.
        `test_idx`: ``numpy.array``
            Array composed of sequencial intergers.

        Return
        ------
        `_train_idx` : ``np.array``
            The new indices for training.
        """
        start_test = horizon.index[test_idx]
        max_test_range = horizon.iloc[test_idx].max()
        left_purge_mask = horizon.iloc[train_idx] <= start_test
        _train_idx = train_idx[left_purge_mask]
        first_event_after_test = horizon.index[horizon.index >= max_test_range]
        if not first_event_after_test.empty:
            embargo_gap = int(self._embargo_ratio * horizon.shape[0])
            right_purge_idx = horizon.index.get_loc[first_event_after_test[0]]
            _train_idx = np.concatenate(
                [
                    _train_idx,
                    train_idx[train_idx >= right_purge_idx + embargo_gap],
                ]
            )
        return _train_idx


class PEKFold(KFold, _BasePEKFold):
    """
    Non-stratified K-fold.

    Parameters
    ----------
    `n_splits`: ``int``
        The number of folds.
    `embargo_ratio`: ``numpy.array``
        Coefficient to be applied in the number of horizons to determine the
        embargo gap.
    """

    def __init__(self, n_splits=5, embargo_ratio=0.1):
        super(PEKFold, self).__init__(
            n_splits=n_splits, shuffle=False, random_state=None
        )
        self._embargo_ratio = embargo_ratio

    def split(self, horizon):
        """
        Split of horizon.

        Parameters
        ----------
        `horizon`: ``pd.Series``
            The timestamps for initial and end of each horizon.
        """
        X = horizon.values
        super_gen = super(PEKFold).split(X)
        for train_idx, test_idx in super_gen:
            train_idx = super(
                PEKFold, self
            )._purge_embargo_squencial_with_test(horizon, train_idx, test_idx)
            yield train_idx, test_idx


class BestEffortStratifiedPEKFold(StratifiedKFold, _BasePEKFold):
    def __init__(self, n_splits=5, embargo_ratio=0.1):
        """
        Stratified K-fold. Due to purge and embargo proecesses, the stratification is
        not perfect, i.e., there will be a difference between the label distributions
        for training and test datasets.

        Parameters
        ----------
        `n_splits`: ``int``
            The number of folds.
        `embargo_ratio`: ``numpy.array``
            Coefficient to be applied in the number of horizons to determine the
            embargo gap.
        """
        super(BestEffortStratifiedPEKFold).__init__(
            n_splits=n_splits, shuffle=False, random_state=None
        )
        self._embargo_ratio = embargo_ratio

    def split(self, horizon, labels):
        """
        Split of horizon.

        Parameters
        ----------
        `horizon`: ``pd.Series``
            The timestamps for initial and end of each horizon.
        """
        X = horizon.values
        super_gen = super(BestEffortStratifiedPEKFold).split(X, labels)
        for train_idx, test_idx in super_gen:
            sequencial_test_idx = np.split(
                test_idx, np.where(np.diff(test_idx) > 1)[0] + 1
            )
            for _test_idx in sequencial_test_idx:
                train_idx = super(
                    BestEffortStratifiedPEKFold, self
                )._purge_embargo_squencial_with_test(
                    horizon, train_idx, _test_idx
                )
            yield train_idx, test_idx
