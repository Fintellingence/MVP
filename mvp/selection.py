import os
from functools import partial
from multiprocessing import Pool
from sklearn.model_selection import KFold, StratifiedKFold

import numpy as np
import pandas as pd

from mvp.bootstrap import sequencial_bootstrap


def parallel_map_df(func, data, num_of_threads, chunk_size, **kwargs):
    """
    Apply `func` in linear distributed chunks of the data in `data`
    using parallel processing.

    Parameters
    ----------
    `func` : ``Called``
        A function to be applied in along the data chunks. It must return a
        ``DataFrame``.
    `data` : ``[Index]``
        The data that will be divided in different chunks.
    ``num_of_threads : ``int``
        The number of threads that will process the chunks.
    ``chunk_size : ``int``
        The size of each chunk
    ``**kwargs : ``dict``
        Addicional arguments that will be passed to the `func`.

    Return
    ------
    `df_out` : ``DataFrame``
        The ``DataFrame`` generated by the application of `func` in
        `data` with the arguments in `**kwargs`.
    """

    def slice_data(chunk_size, data_size):
        n_chunks = int(np.round(data_size / chunk_size)) + 1
        chunk_idx = np.floor(np.linspace(0, len(data), n_chunks)).astype(int)
        for i in range(1, chunk_idx.size):
            yield data[chunk_idx[i - 1] : chunk_idx[i]]

    data_size = len(data) 
    partial_func = partial(func, **kwargs)
    if chunk_size < data_size:
        slicer = slice_data(chunk_size, data_size)
        with Pool(num_of_threads) as pool:
            output = [out for out in pool.imap(partial_func, slicer)]
        pd_out = pd.concat(output, axis=0)
        pd_out = pd_out.loc[~pd_out.index.duplicated(keep="last")].sort_index()
    else:
        pd_out = partial_func(data)
    return pd_out


def chunk_count_occurrences(event_chunks, closed_index, horizon):
    """
    Determine the number of occurrences of horizons in `event_chunks`.

    Parameters
    ----------
    `closed_index` : ``Index``
        The sorted timestamps of the closed prices
    `horizon` : ``Series``
        The start and end of each horizon
    `event_chunks` : ``[list, Series, Index]``
        The timestamps that compose the events of interest

    Return
    ------
    count : ``Series``
        The number of occurrence of the `event_chunks` in all horizons
    """
    horizon = horizon[: event_chunks[-1]]
    horizon = horizon[horizon >= event_chunks[0]]
    idx_of_interest = closed_index.searchsorted(
        [horizon.index[0], horizon.max()]
    )
    count = pd.Series(
        0, index=closed_index[idx_of_interest[0] : idx_of_interest[1] + 1]
    )
    for s, e in horizon.iteritems():
        count.loc[s:e] += 1
    return count.loc[event_chunks[0] : horizon[event_chunks].max()]


def chunk_avg_uniqueness(event_chunks, horizon, occurrences):
    """
    Determine the average uniqueness of `horizon`, i.e., the average
    uniqueness of labels, in `event_chunks`.

    Parameters
    ----------
    `horizon` : ``Series``
        The start and end of each horizon
    `occurrences` : ``Series``
        The number of occurrence of all horizons in all events
        (see ``chunk_count_occurrences``)
    `event_chunks` : ``[list, Series, Index]``
        The timestamps that compose the events of interest

    Return
    ------
    avg_uniqueness : ``Series``
        Average uniquess associated with `horizon` in `event_chunks`
    """
    avg_uniqueness = pd.Series(index=event_chunks)
    for s, e in horizon.loc[event_chunks].iteritems():
        avg_uniqueness.loc[s] = (1.0 / occurrences.loc[s:e]).mean()
    return avg_uniqueness


def chunk_sample_weights(event_chunks, occurrences, horizon, closed_prices):
    """
    Determine the weights based on returns and number of instant overlaps,
    for horizons that start into `event_chunks`.

    Parameters
    ----------
    `occurrences` : ``Series``
        The number of occurrence of all horizons in all events
        (see ``event_chunks``)
    `horizon` : ``Series``
        The start and end of each horizon.
    `closed_prices` : ``Series``
        Closed prices for a symbol, index is composed of time instants.
    `event_chunks` : ``[list, Series, Index]``
        The timestamps that compose the events of interest

    Return
    ------
    weights : ``Series``
        Weights for `event_chunks`
    """
    log_return = np.log(closed_prices).diff()
    weights = pd.Series(index=event_chunks)
    horizon = horizon.loc[event_chunks]
    for s, e in horizon.iteritems():
        weights[s] = (log_return.loc[s:e] / occurrences.loc[s:e]).sum()
    return weights.abs()


def count_occurrences(closed_index, horizon, num_of_threads, chunck_size):
    """
    Compute all occurrences into the event space.
    """
    events = horizon.index
    occurrences = parallel_map_df(
        chunk_count_occurrences,
        events,
        num_of_threads,
        chunck_size,
        horizon=horizon,
        closed_index=closed_index,
    )
    occurrences = occurrences.reindex(closed_index).fillna(0)
    return occurrences


def avg_uniqueness(occurrences, horizon, num_of_threads, chunck_size):
    """
    Compute all average uniqueness into the event space.
    """
    events = horizon.index
    avg_uniqueness = parallel_map_df(
        chunk_avg_uniqueness,
        events,
        num_of_threads,
        chunck_size,
        horizon=horizon,
        occurrences=occurrences,
    )
    return avg_uniqueness


def sample_weights(
    occurrences, horizon, closed_prices, num_of_threads, chunck_size
):
    """
    Compute weights for all events
    """
    events = horizon.index
    sample_weights = parallel_map_df(
        chunk_sample_weights,
        events,
        num_of_threads,
        chunck_size,
        horizon=horizon,
        occurrences=occurrences,
        closed_prices=closed_prices,
    )
    return sample_weights


def time_weights(avg_uniqueness, p=.25, min_ratio=0.01, sort=False):
    """
    Determine the weights based on time evolution (the newest ones have larger
    weights). The time evolution is represented by the cummulative sum of
    average uniqueness for all events.

    Parameters
    ----------
    `avg_uniqueness` : ``Series``
        The average uniqueness for all events
    `p` : ``float``
        The line parameter in [0,1]
    `min_ratio` : ``float``
        The ratio to be multiplied by the maximum weitgh
    `sort` : ``bool``
        The inidication to sort the `avg_uniqueness` Serie

    Return
    ------
    weights : ``Series``
        Weights for time evolution based on average uniqueness
    """
    if sort:
        avg_uniqueness = avg_uniqueness.sort_index()
    weighted_time = avg_uniqueness.cumsum()
    a = (1.0 - p) / weighted_time.iloc[-1]
    b = 1.0 - a * weighted_time.iloc[-1]
    weights = a * weighted_time + b
    min_weight = weights.max() * min_ratio
    weights[weights < 0] = min_weight
    return weights


def raw_horizon(closed_idx, horizon):
    """
    Determine a matrix to indicate the events and their horizons based on
    indices, instead of timestamps.
    """
    table = pd.Series(
        np.arange(closed_idx.shape[0], dtype=np.int32), index=closed_idx
    )
    raw_horizon = np.zeros((horizon.shape[0], 2), dtype=np.int32)
    for i, (s, e) in enumerate(horizon.iteritems()):
        raw_horizon[i][0] = table[s]
        raw_horizon[i][1] = table[e]
    return raw_horizon


def bootstrap_selection(
    np_horizon, num_of_data=-1, random_state=None, num_of_threads=None
):
    """
    Select `num_of_data` horizons by sampling with replacement
    (aka bootstrap sampling) sequentialy. The samples are drawn from a
    discrete distribution, in which the probabilities of occurrence of each
    horizon rely on the number of overlapping with the past sampled
    horizons.

    Parameters
    ----------
    `np_horizon` : ``np.array``
        The matrix indicating the events and their horizons using integer indices
        (see ``raw_horizon`` function)

    Return
    ------
    data_idx : ``np.array``
        The list contaning the indices for selected events
    """
    num_of_threads = (
        num_of_threads if num_of_threads is not None else os.cpu_count()
    )
    return sequencial_bootstrap(
        np_horizon, num_of_threads, num_of_data, random_state
    )


# FIXME: Not Good
class _BasePEKFold:
    """
    Base class for cross validation based on K-fold for both non-stratified
    and stratified
    """

    def _purge_embargo_squencial_with_test(
        self, horizon, train_idx, test_idx, window=0
    ):
        """
        Apply the purge and embargo processes to the `train_idx` based on the
        `test_idx` with sequencial indices representing a range of test events.
        A possible rolling window is also considered to avoid data leakage.

        Parameters
        ----------
        `train_idx`: ``np.array``
            Array composed of intergers.
        `test_idx`: ``numpy.array``
            Array composed of sequencial intergers.
        `window`: ``int``
            Size of the window used to get the rolling statistics.

        Return
        ------
        `_train_idx` : ``np.array``
            The new indices for training.
        """
        test_events = horizon.index[test_idx]
        left_purge_mask = horizon.iloc[train_idx] <= test_events.min()
        left_train_idx = train_idx[left_purge_mask]
        if left_train_idx.shape[0] > 0 and window > 0:
            left_gap = (
                (train_idx > left_train_idx[-1]) & (train_idx < test_idx[0])
            ).sum()
            if left_gap < window:
                test_idx = test_idx[window - left_gap - 1 :]
                if test_idx.shape[0] == 0:
                    print(
                        "The number of dropped test indices are greater than"
                        " the fold size. Try a small number of splits"
                    )
                    return np.array([]), np.array([])
        max_test_range = horizon.iloc[test_idx].max()
        first_event_after_test = horizon.index[horizon.index >= max_test_range]
        if not first_event_after_test.empty:
            first_event_after_test = first_event_after_test[0]
            embargo_gap = int(self._embargo_ratio * horizon.shape[0])
            right_purge_idx = horizon.index.get_loc(first_event_after_test)
            right_train_idx = train_idx[
                train_idx >= right_purge_idx + embargo_gap
            ]
            if right_train_idx.shape[0] > 0 and window > 0:
                right_gap = (
                    (train_idx < right_train_idx[0])
                    & (train_idx > test_idx[-1])
                ).sum()
                if right_gap < window:
                    right_train_idx = right_train_idx[window - right_gap - 1 :]
            train_idx = np.concatenate([left_train_idx, right_train_idx])
            if train_idx.shape[0] == 0:
                print(
                    "The number of dropped train indices are greater than"
                    " the fold size. Try a small number of splits"
                )
                return np.array([]), np.array([])
        return train_idx, test_idx


class PEKFold(KFold, _BasePEKFold):
    """
    Non-stratified K-fold.

    Parameters
    ----------
    `n_splits`: ``int``
        The number of folds.
    `embargo_ratio`: ``numpy.array``
        Coefficient to be applied in the number of horizons to determine the
        embargo gap.
    """

    def __init__(self, n_splits=5, embargo_ratio=0.1):
        super(PEKFold, self).__init__(
            n_splits=n_splits, shuffle=False, random_state=None
        )
        self._embargo_ratio = embargo_ratio

    def split(self, horizon):
        """
        Split of horizon.

        Parameters
        ----------
        `horizon`: ``pd.Series``
            The timestamps for initial and end of each horizon.
        """
        X = horizon.values
        super_gen = super(PEKFold, self).split(X)
        for train_idx, test_idx in super_gen:
            train_idx, test_idx = super(
                PEKFold, self
            )._purge_embargo_squencial_with_test(horizon, train_idx, test_idx)
            if train_idx.shape[0] == 0:
                continue
            yield train_idx, test_idx


class BestEffortStratifiedPEKFold(StratifiedKFold, _BasePEKFold):
    def __init__(self, n_splits=5, embargo_ratio=0.1):
        """
        Stratified K-fold. Due to purge and embargo proecesses, the stratification is
        not perfect, i.e., there will be a difference between the label distributions
        for training and test datasets.

        Parameters
        ----------
        `n_splits`: ``int``
            The number of folds.
        `embargo_ratio`: ``numpy.array``
            Coefficient to be applied in the number of horizons to determine the
            embargo gap.
        """
        super(BestEffortStratifiedPEKFold).__init__(
            n_splits=n_splits, shuffle=False, random_state=None
        )
        self._embargo_ratio = embargo_ratio

    def split(self, horizon, labels):
        """
        Split of horizon.

        Parameters
        ----------
        `horizon`: ``pd.Series``
            The timestamps for initial and end of each horizon.
        """
        X = horizon.values
        super_gen = super(BestEffortStratifiedPEKFold, self).split(X, labels)
        for train_idx, test_idx in super_gen:
            sequencial_test_idx = np.split(
                test_idx, np.where(np.diff(test_idx) > 1)[0] + 1
            )
            for _test_idx in sequencial_test_idx:
                train_idx, test_idx = super(
                    BestEffortStratifiedPEKFold, self
                )._purge_embargo_squencial_with_test(
                    horizon, train_idx, _test_idx
                )
                if train_idx.shape[0] == 0:
                    continue
            yield train_idx, test_idx
