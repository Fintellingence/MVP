from functools import partial
from multiprocessing import Pool

import numpy as np
import pandas as pd


def parallel_map_df(func, data, num_of_threads, chunk_size, **kwargs):
    """
    Apply `func` in linear distributed chunks of the data in `data`
    using parallel processing.

    Parameters
    ----------
    `func` : ``Called``
        A function to be applied in along the data chunks. It must return a
        ``DataFrame``.
    `data` : ``[DataFrame, Series]``
        The data that will be divided in different chunks.
    ``num_of_threads : ``int``
        The number of threads that will process the chunks.
    ``chunk_size : ``int``
        The size of the chunk.
    ``**kwargs : ``dict``
        Addicional arguments that will be passed to the `func`.

    Return
    ------
    `df_out` : ``DataFrame``
        The ``DataFrame`` generated by the application of `func` in
        `data` with the arguments in `**kwargs`.
    """

    def slice_data(chunk, data):
        chunk_idx = np.ceil(np.linspace(0, len(data), chunk)).astype(int)
        for i in range(1, chunk_idx.size):
            yield data[chunk_idx[i - 1 : i]]

    slicer = slice_data(chunk_size, data)
    partial_func = partial(func, **kwargs)
    with Pool(num_of_threads) as pool:
        output = [out for out in pool.imap_unordered(partial_func, slicer)]
    df_out = pd.concat(output, axis=0).sort_index()
    if np.any(df_out.index.duplicated()):
        raise RuntimeError("Duplicated index.")
    return df_out


def interval_count_occurrences(closed_index, horizon, interval):
    """
    Determine the number of occurrences of horizons in `interval`.

    Parameters
    ----------
    `closed_index` : ``Index``
        The sorted timestamps of the closed prices
    `horizon` : ``DataFrame``
        The start and end of each horizon
    `interval` : ``[list, Series, Index]``
        The timestamps that compose the interval of interest

    Return
    ------
    count : ``Series``
        The number of occurrence of the `interval` in all horizons
    """
    horizon = horizon.loc[
        (horizon["start"] <= interval[-1])
        & (horizon["end"] >= interval[0])
    ]
    idx_of_interest = closed_index.searchsorted(
        [horizon["start"].min(), horizon["end"].max()]
    )
    count = pd.Series(
        0, index=closed_index[idx_of_interest[0] : idx_of_interest[1] + 1]
    )
    horizon_np = horizon.values
    for s, e in horizon_np:
        count.loc[s:e] += 1
    return count.loc[interval[0] : interval[-1]]


def interval_avg_uniqueness(horizon, occurrences, interval):
    """
    Determine the average uniqueness of `horizon`, i.e., the average
    uniqueness of labels, in `interval`.

    Parameters
    ----------
    `horizon` : ``DataFrame``
        The start and end of each horizon
    `occurrences` : ``Series``
        The number of occurrence of all horizons in all events
        (see ``interval_count_occurrences``)
    `interval` : ``[list, Series, Index]``
        The timestamps that compose the interval of interest

    Return
    ------
    avg_uniqueness : ``Series``
        Average uniquess associated with `horizon` in `interval`
    """
    avg_uniqueness = pd.Series(index=interval)
    horizon = horizon.loc[
        (horizon["start"] >= interval[0])
        & (horizon["end"] <= interval[-1])
    ]
    horizon_np = horizon.values
    for s, e in horizon_np:
        avg_uniqueness.loc[s] = (1.0 / occurrences.loc[s:e]).mean()
    return avg_uniqueness


def interval_sample_weights(occurrences, horizon, closed_prices, interval):
    """
    Determine the weights based on returns and number of instant overlaps,
    for horizons that start into `interval`.

    Parameters
    ----------
    `occurrences` : ``Series``
        The number of occurrence of all horizons in all events
        (see ``interval_count_occurrences``)
    `horizon` : ``DataFrame``
        The start and end of each horizon.
    `closed_prices` : ``Series``
        Closed prices for a symbol, index is composed of time instants.
    `interval` : ``[list, Series, Index]``
        The timestamps that compose the interval of interest

    Return
    ------
    weights : ``Series``
        Weights for `interval`
    """
    log_return = np.log(closed_prices).diff()
    weights = pd.Series(index=interval)
    horizon_np = horizon[horizon["start"] == interval].values
    for s, e in horizon_np:
        weights[s] = (log_return.loc[s:e] / occurrences.loc[s:e]).sum()
    return weights.abs()


def count_occurences(closed_index, horizon, num_of_threads, chunk_size):
    """
    Compute all occurrences into the event space.
    """
    events = horizon["start"]
    occurrences = parallel_map_df(
        interval_count_occurrences,
        events,
        num_of_threads,
        chunk_size,
        horizon=horizon,
        closed_index=closed_index,
    )
    occurrences = occurrences.reindex(closed_index).fillna(0)
    return occurrences


def avg_uniqueness(occurrences, horizon, num_of_threads, chunk_size):
    """
    Compute all average uniqueness into the event space.
    """
    events = horizon["start"]
    avg_uniqueness = parallel_map_df(
        interval_avg_uniqueness,
        events,
        num_of_threads,
        chunk_size,
        horizon=horizon,
        ocurrences=occurrences,
    )
    return avg_uniqueness


def sample_weights(occurrences, horizon, closed_prices, num_of_threads, chunk_size):
    """
    Compute weights for all events
    """
    events = horizon["start"]
    avg_uniqueness = parallel_map_df(
        interval_sample_weights,
        events,
        num_of_threads,
        chunk_size,
        horizon=horizon,
        ocurrences=occurrences,
        closed_prices=closed_prices,
    )
    return avg_uniqueness


def time_weights(avg_uniqueness, p=1):
    """
    Determine the weights based on time evolution (the newest ones have greater
    weights). The time evolution is represented by the cummulative sum of
    average uniqueness for all events.

    Parameters
    ----------
    `avg_uniqueness` : ``Series``
        The average uniqueness for all events

    Return
    ------
    weights : ``Series``
        Weights for time evolution based on average uniqueness
    """
    weighted_time = avg_uniqueness.sort_index().cumsum()
    a = (1.0 - p) / weighted_time.iloc[-1]
    b = 1.0 - slope * weighted_time.iloc[-1]
    weights = a * weighted_time + b
    weights[weights < 0] = 0
    return weights


def indicator(closed_idx, horizon):
    """
    Determine a matrix to indicate of occurrences, in which rows
    represent the timestamps and columns represent the events
    (aka horizons).
    """
    indicator = pd.DataFrame(
        0, index=closed_idx, columns=range(horizon.shape[0])
    )
    horizon_np = horizon.values
    for i, (s, e) in enumerate(horizon_np):
        indicator.loc[s:e, i] = 1.0
    return indicator


def bootstrap_avg_uniqueness(indicator):
    """
    Determine the average uniqueness for the events represented in
    indicator matrix.
    """
    sum_occurrences = indicator.sum(axis=1)
    uniqueness = indicator.div(sum_occurrences, axis=0)
    return uniqueness[uniqueness > 0].mean()


def bootstrap_selection(indicator, num_of_data=None, seed=12345):
    """
    Select `num_of_data` horizons by sampling with replacement
    (aka bootstrap sampling) sequentialy. The samples are drawn from a
    discrete distribution, in which the probabilities of occurrence of each
    horizon rely on the number of overlapping with the past sampled
    horizons.

    Parameters
    ----------
    `indicator` : ``DataFrame``
        The matrix to indicate of occurrences of horizons along the time
        space of closed prices

    Return
    ------
    data_idx : ``list``
        The list contaning the indices for selected events (aka horizons)
    """
    data_idx = []
    rng = np.random.default_rng(seed)
    num_of_events = indicator.shape[1]
    num_of_data = num_of_events if num_of_data is None else num_of_data
    for _ in range(num_of_data):
        avg_uniqueness = []
        for i in range(num_of_events):
            occurrences_of_interest = indicator[data_idx + [i]]
            avg_uniqueness.append(
                bootstrap_avg_uniqueness(occurrences_of_interest)
            )
        prob = avg_uniqueness / avg_uniqueness.sum()
        data_idx.append(rng.choice(indicator.columns, p=prob))
    return data_idx


def cross_validation():
    pass
